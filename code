1)


from mrjob.job import MRJob
import re

WORD_RE = re.compile(r"[\w']+")

class WordCount(MRJob):

    def mapper(self, _, line):
        for word in WORD_RE.findall(line):
            yield word.lower(), 1

    def reducer(self, word, counts):
        yield word, sum(counts)

if __name__ == '__main__':
    WordCount.run()


2)

from mrjob.job import MRJob
import re

# Regular expression pattern to extract words from text
WORD_RE = re.compile(r"[\w']+")

class NonStopwordWordCount(MRJob):

    def configure_args(self):
        super(NonStopwordWordCount, self).configure_args()
        self.add_passthru_arg('--stopwords', default='', help='Path to stopwords file')

    def mapper(self, _, line):
        # Tokenize the line into words
        words = WORD_RE.findall(line)

        # Filter out stopwords and emit each non-stopword word with a count of 1
        for word in words:
            word_lower = word.lower()
            if word_lower not in self.get_stopwords():
                yield word_lower, 1

    def combiner(self, word, counts):
        # Combine the counts for each word
        yield word, sum(counts)

    def reducer(self, word, counts):
        # Sum the counts for each word and emit the result
        total_count = sum(counts)
        yield word, total_count

    def get_stopwords(self):
        # Load stopwords from a file specified via command-line argument
        stopwords_path = self.options.stopwords
        if stopwords_path:
            with open(stopwords_path, 'r') as stopwords_file:
                return set(stopwords_file.read().split())
        else:
            return set()

if __name__ == '__main__':
    NonStopwordWordCount.run()

3)
from mrjob.job import MRJob
import re

# Regular expression pattern to extract words from text
WORD_RE = re.compile(r"[\w']+")

class BigramCount(MRJob):

    def mapper(self, _, line):
        # Tokenize the line into words
        words = WORD_RE.findall(line)

        # Emit word bigrams as key-value pairs
        for i in range(len(words) - 1):
            bigram = f"{words[i]},{words[i+1]}"
            yield bigram, 1

    def combiner(self, bigram, counts):
        # Combiner to aggregate counts for each bigram before shuffling
        yield bigram, sum(counts)

    def reducer(self, bigram, counts):
        # Sum up the counts for each bigram to get the total occurrences
        yield bigram, sum(counts)

if __name__ == '__main__':
    BigramCount.run()

4)


import sys
from collections import defaultdict

def generate_inverted_index(document_id, text):
    words = text.strip().split()
    inverted_index = defaultdict(list)
    for word in words:
        inverted_index[word].append(document_id)
    return inverted_index

def main():
    for line in sys.stdin:
        parts = line.strip().split(":")
        if len(parts) != 2:
            continue

        document_id, text = map(str.strip, parts)

        inverted_index = generate_inverted_index(document_id, text)

        for word, documents in inverted_index.items():
            document_list = ", ".join(documents)
            print(f'"{word}" "{document_list}"')

if __name__ == "__main__":
    main()
